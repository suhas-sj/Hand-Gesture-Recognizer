{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from collections import deque\n",
    "from scipy.interpolate import interp1d\n",
    "from previous_lesson1 import (detectHandsLandmarks, recognizeGestures,\n",
    "                             calculateDistance,changeSatValue,\n",
    "                             changeContrast, gammaCorrection,draw,extractPoseKeypoints,drawShapes,selectShape)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping \n",
    "from tensorflow.keras.utils import plot_model\n",
    "import os\n",
    "import numpy as np\n",
    "import subprocess\n",
    "def open_ipynb_file(file_path):\n",
    "    os.system(f\"jupyter-notebook {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'mediapipe.python.solutions.hands' from 'c:\\\\Users\\\\Bhavith\\\\.conda\\\\envs\\\\finalYear\\\\lib\\\\site-packages\\\\mediapipe\\\\python\\\\solutions\\\\hands.py'>\n"
     ]
    }
   ],
   "source": [
    "# Initialize the mediapipe hands class.\n",
    "mp_hands = mp.solutions.hands\n",
    "print(mp_hands)\n",
    "# Set up the Hands functions for images and videos.\n",
    "hands = mp_hands.Hands(static_image_mode=True, max_num_hands=2, min_detection_confidence=0.3)\n",
    "hands_videos = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.8)\n",
    "\n",
    "# Initialize the mediapipe drawing class.\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "# website_opened1=False\n",
    "# website_opened2=False\n",
    "# calc_opened=False\n",
    "# chrome_opened=False\n",
    "# notepad_opened=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "camera_video.isOpened()=True\n",
      "current_gesture=SPIDERMAN\n",
      "current_gesture=SPIDERMAN\n",
      "current_gesture=SPIDERMAN\n",
      "current_gesture=SPIDERMAN\n",
      "current_gesture=SPIDERMAN\n",
      "current_gesture=SPIDERMAN\n",
      "current_gesture=SPIDERMAN\n",
      "current_gesture=SPIDERMAN\n",
      "current_gesture=SPIDERMAN\n",
      "current_gesture=SPIDERMAN\n",
      "current_gesture=SPIDERMAN\n",
      "current_gesture=SPIDERMAN\n",
      "current_gesture=SPIDERMAN\n",
      "current_gesture=SPIDERMAN\n",
      "current_gesture=SPIDERMAN\n",
      "current_gesture=SPIDERMAN\n",
      "current_gesture=SPIDERMAN\n",
      "current_gesture=SPIDERMAN\n",
      "current_gesture=SPIDERMAN\n",
      "current_gesture=SPIDERMAN\n",
      "{'THUMB': (721, 482), 'INDEX': (666, 251), 'MIDDLE': (718, 507), 'RING': (750, 524), 'PINKY': (787, 538)}\n",
      "{'THUMB': (719, 484), 'INDEX': (665, 250), 'MIDDLE': (718, 508), 'RING': (750, 524), 'PINKY': (786, 536)}\n",
      "{'THUMB': (715, 485), 'INDEX': (661, 251), 'MIDDLE': (713, 511), 'RING': (745, 527), 'PINKY': (781, 538)}\n",
      "{'THUMB': (695, 489), 'INDEX': (655, 256), 'MIDDLE': (701, 515), 'RING': (735, 533), 'PINKY': (772, 545)}\n",
      "{'THUMB': (687, 501), 'INDEX': (637, 266), 'MIDDLE': (689, 527), 'RING': (726, 544), 'PINKY': (762, 554)}\n",
      "{'THUMB': (663, 520), 'INDEX': (578, 292), 'MIDDLE': (674, 542), 'RING': (715, 553), 'PINKY': (751, 564)}\n",
      "{'THUMB': (658, 541), 'INDEX': (552, 330), 'MIDDLE': (675, 560), 'RING': (717, 574), 'PINKY': (753, 585)}\n",
      "{'THUMB': (689, 563), 'INDEX': (614, 324), 'MIDDLE': (695, 578), 'RING': (733, 591), 'PINKY': (769, 606)}\n",
      "{'THUMB': (704, 577), 'INDEX': (690, 337), 'MIDDLE': (708, 599), 'RING': (746, 606), 'PINKY': (777, 620)}\n",
      "{'THUMB': (685, 581), 'INDEX': (623, 386), 'MIDDLE': (697, 606), 'RING': (737, 611), 'PINKY': (764, 615)}\n",
      "{'THUMB': (691, 568), 'INDEX': (585, 413), 'MIDDLE': (691, 586), 'RING': (732, 592), 'PINKY': (767, 601)}\n",
      "{'THUMB': (704, 561), 'INDEX': (630, 421), 'MIDDLE': (705, 576), 'RING': (744, 582), 'PINKY': (776, 595)}\n",
      "{'THUMB': (717, 556), 'INDEX': (648, 354), 'MIDDLE': (719, 578), 'RING': (753, 584), 'PINKY': (786, 600)}\n",
      "{'THUMB': (732, 540), 'INDEX': (678, 305), 'MIDDLE': (729, 559), 'RING': (762, 574), 'PINKY': (801, 588)}\n",
      "{'THUMB': (731, 531), 'INDEX': (662, 297), 'MIDDLE': (730, 549), 'RING': (763, 563), 'PINKY': (798, 577)}\n"
     ]
    }
   ],
   "source": [
    "# Initialize the VideoCapture object to read from the webcam.\n",
    "camera_video = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "camera_video.set(3,1280)\n",
    "camera_video.set(4,960)\n",
    "website_opened1=False\n",
    "website_opened2=False\n",
    "# Create named window for resizing purposes.\n",
    "cv2.namedWindow('Live Gesture Control Application', cv2.WINDOW_NORMAL)\n",
    "\n",
    "# Initialize the minimum brightness and saturation scale factors.\n",
    "min_brightness = min_saturation = 0\n",
    "\n",
    "# Initialize the maximum brightness and saturation scale factors.\n",
    "max_brightness = max_saturation = 3\n",
    "\n",
    "# Initialize the minimum and maximum contrast scale factors. \n",
    "min_contrast=1\n",
    "max_contrast=3\n",
    "\n",
    "# Initialize the minimum and maximum gamma scale factors.\n",
    "min_gamma=0.1\n",
    "max_gamma=3\n",
    "\n",
    "# Initialize a buffer to store recognized gestures.\n",
    "buffer = deque([], maxlen=20)\n",
    "\n",
    "# # Initialize some variables to store the change state.\n",
    "# change_brightness = change_saturation = change_contrast = change_gamma = False\n",
    "\n",
    "# Initialize a variable to store the current mode.\n",
    "current_mode = ''\n",
    "\n",
    "print(f\"camera_video.isOpened()={camera_video.isOpened()}\")\n",
    "# Iterate until the webcam is accessed successfully.\n",
    "while camera_video.isOpened():\n",
    "   \n",
    "    # Read a frame.\n",
    "    ok, frame = camera_video.read()\n",
    "    \n",
    "    # Check if frame is not read properly then continue to the next iteration to read the next frame.\n",
    "    if not ok:\n",
    "#         print(\"Here\")\n",
    "        continue\n",
    "    \n",
    "    # Flip the frame horizontally for natural (selfie-view) visualization.\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    # Get the height and width of the frame of the webcam video.\n",
    "    frame_height, frame_width, _ = frame.shape\n",
    "    \n",
    "    # Perform Hands landmarks detection on the frame.\n",
    "    _, results = detectHandsLandmarks(frame, hands_videos, draw=True, display=False)\n",
    "    \n",
    "    # Check if the hands landmarks in the frame are detected.\n",
    "    if results.multi_hand_landmarks:\n",
    "    \n",
    "        # Perform left hand gesture recognition.\n",
    "        current_gesture, size_gesture_tip_pts = recognizeGestures(frame, results,\n",
    "                                                                  hand_label='LEFT',\n",
    "                                                                  draw=False, display=False)\n",
    "        \n",
    "        # Check if a known gesture is recognized.\n",
    "        if current_gesture != 'UNKNOWN':\n",
    "            print(f\"current_gesture={current_gesture}\")\n",
    "            # Check if all the gestures stored in the buffer are equal to the current gesture.\n",
    "            if all(current_gesture==gesture for gesture in buffer):\n",
    "                \n",
    "                # Append the current gesture into the buffer.\n",
    "                buffer.append(current_gesture)\n",
    "                \n",
    "            # Otherwise.\n",
    "            else:\n",
    "                \n",
    "                # Clear the buffer.\n",
    "                buffer.clear()\n",
    "            \n",
    "            # Check if the length of the buffer is equal to 20.\n",
    "            if len(buffer) == 20:\n",
    "                \n",
    "                # Calculate the distance between the middle finger tip and thumb tip landmark of the right hand.\n",
    "                # distance = calculateDistance(frame, size_gesture_tip_pts['INDEX'],\n",
    "                #                              size_gesture_tip_pts['THUMB'], display=False)\n",
    "                # print(distance)\n",
    "                # Check if the distance is calculated successfully.\n",
    "                # This will be none in case when the hand is not in the frame.\n",
    "                if current_gesture != 'UNKNOWN':\n",
    "                    \n",
    "                    # Check if the current hand gesture is INDEX POINTING UP.\n",
    "                    if current_gesture == 'INDEX POINTING UP':\n",
    "                        # Initialize the mediapipe hands class.\n",
    "                        mp_hands = mp.solutions.hands\n",
    "                        print(mp_hands)\n",
    "                        # Set up the Hands functions for images and videos.\n",
    "                        hands = mp_hands.Hands(static_image_mode=True, max_num_hands=2, min_detection_confidence=0.3)\n",
    "                        hands_videos = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.8)\n",
    "\n",
    "                        # Initialize the mediapipe drawing class.\n",
    "                        mp_drawing = mp.solutions.drawing_utils\n",
    "                        # Initialize the VideoCapture object to read from the webcam.\n",
    "                        camera_video = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "                        camera_video.set(3,1280)\n",
    "                        camera_video.set(4,960)\n",
    "                        website_opened1=False\n",
    "                        website_opened2=False\n",
    "                        calc_opened=False\n",
    "                        chrome_opened=False\n",
    "                        notepad_opened=False\n",
    "                        \n",
    "                        # Create named window for resizing purposes.\n",
    "                        cv2.namedWindow('Live Gesture Control Application', cv2.WINDOW_NORMAL)\n",
    "\n",
    "                        # Initialize the minimum brightness and saturation scale factors.\n",
    "                        min_brightness = min_saturation = 0\n",
    "\n",
    "                        # Initialize the maximum brightness and saturation scale factors.\n",
    "                        max_brightness = max_saturation = 3\n",
    "\n",
    "                        # Initialize the minimum and maximum contrast scale factors. \n",
    "                        min_contrast=1\n",
    "                        max_contrast=3\n",
    "\n",
    "                        # Initialize the minimum and maximum gamma scale factors.\n",
    "                        min_gamma=0.1\n",
    "                        max_gamma=3\n",
    "\n",
    "                        # Initialize a buffer to store recognized gestures.\n",
    "                        buffer = deque([], maxlen=20)\n",
    "\n",
    "                        # Initialize some variables to store the change state.\n",
    "                        change_brightness = change_saturation = change_contrast = change_gamma = False\n",
    "\n",
    "                        # Initialize a variable to store the current mode.\n",
    "                        current_mode = ''\n",
    "\n",
    "                        print(f\"camera_video.isOpened()={camera_video.isOpened()}\")\n",
    "                        # Iterate until the webcam is accessed successfully.\n",
    "                        while camera_video.isOpened():\n",
    "                        \n",
    "                            # Read a frame.\n",
    "                            ok, frame = camera_video.read()\n",
    "                            \n",
    "                            # Check if frame is not read properly then continue to the next iteration to read the next frame.\n",
    "                            if not ok:\n",
    "                        #         print(\"Here\")\n",
    "                                continue\n",
    "                            \n",
    "                            # Flip the frame horizontally for natural (selfie-view) visualization.\n",
    "                            frame = cv2.flip(frame, 1)\n",
    "                            \n",
    "                            # Get the height and width of the frame of the webcam video.\n",
    "                            frame_height, frame_width, _ = frame.shape\n",
    "                            \n",
    "                            # Perform Hands landmarks detection on the frame.\n",
    "                            _, results = detectHandsLandmarks(frame, hands_videos, draw=True, display=False)\n",
    "                            \n",
    "                            # Check if the hands landmarks in the frame are detected.\n",
    "                            if results.multi_hand_landmarks:\n",
    "                            \n",
    "                                # Perform left hand gesture recognition.\n",
    "                                current_gesture, size_gesture_tip_pts = recognizeGestures(frame, results,\n",
    "                                                                                        hand_label='RIGHT',\n",
    "                                                                                        draw=False, display=False)\n",
    "                                \n",
    "                                # Check if a known gesture is recognized.\n",
    "                                if current_gesture != 'UNKNOWN':\n",
    "                                    print(f\"current_gesture={current_gesture}\")\n",
    "                                    # Check if all the gestures stored in the buffer are equal to the current gesture.\n",
    "                                    if all(current_gesture==gesture for gesture in buffer):\n",
    "                                        \n",
    "                                        # Append the current gesture into the buffer.\n",
    "                                        buffer.append(current_gesture)\n",
    "                                        \n",
    "                                    # Otherwise.\n",
    "                                    else:\n",
    "                                        \n",
    "                                        # Clear the buffer.\n",
    "                                        buffer.clear()\n",
    "                                    \n",
    "                                    # Check if the length of the buffer is equal to 20.\n",
    "                                    if len(buffer) == 20:\n",
    "                                        \n",
    "                                        # Calculate the distance between the middle finger tip and thumb tip landmark of the right hand.\n",
    "                                        # distance = calculateDistance(frame, size_gesture_tip_pts['INDEX'],\n",
    "                                        #                              size_gesture_tip_pts['THUMB'], display=False)\n",
    "                                        # print(distance)\n",
    "                                        # Check if the distance is calculated successfully.\n",
    "                                        # This will be none in case when the hand is not in the frame.\n",
    "                                        if current_gesture != 'UNKNOWN':\n",
    "                                            \n",
    "                                            # Check if the current hand gesture is INDEX POINTING UP.\n",
    "                                            if current_gesture == 'INDEX POINTING UP':\n",
    "                                                distance = calculateDistance(frame, size_gesture_tip_pts['INDEX'],\n",
    "                                                size_gesture_tip_pts['THUMB'], display=False)\n",
    "                                                # Get the interpolation function.\n",
    "                                                brightness_interp_f = interp1d([30,230], [min_brightness, max_brightness])\n",
    "                                                \n",
    "                                                # Calculate the brighness scale factor based on the calculated distance.\n",
    "                                                # Higher the distance, higher the brighness scale factor will be.\n",
    "                                                brightness_scale = brightness_interp_f(distance)\n",
    "                                                \n",
    "                                                # Update the current mode and change state.\n",
    "                                                current_mode = 'BRIGHTNESS'\n",
    "                                                change_brightness = True\n",
    "                                            \n",
    "                                            # Check if the current hand gesture is VICTORY.\n",
    "                                            elif current_gesture == 'VICTORY':\n",
    "                                                distance = calculateDistance(frame, size_gesture_tip_pts['INDEX'],\n",
    "                                                                    size_gesture_tip_pts['MIDDLE'], display=False)\n",
    "                                                # Get the interpolation function and calculate the saturation scale factor.\n",
    "                                                saturation_interp_f = interp1d([30,230], [min_saturation, max_saturation])\n",
    "                                                saturation_scale = saturation_interp_f(distance)\n",
    "                                                \n",
    "                                                # Update the current mode and change state.\n",
    "                                                current_mode = 'SATURATION'\n",
    "                                                change_saturation = True\n",
    "                                            \n",
    "                                            # Check if the current hand gesture is SPIDERMAN.\n",
    "                                            elif current_gesture == 'THREE':\n",
    "                                                distance = calculateDistance(frame, size_gesture_tip_pts['INDEX'],\n",
    "                                                                    size_gesture_tip_pts['RING'], display=False)\n",
    "                                                # Get the interpolation function and calculate the contrast scale factor.\n",
    "                                                contrast_interp_f = interp1d([30,230], [min_contrast, max_contrast])\n",
    "                                                contrast_scale = contrast_interp_f(distance)\n",
    "                                                \n",
    "                                                # Update the current mode and change state.\n",
    "                                                current_mode = 'CONTRAST'\n",
    "                                                change_contrast = True\n",
    "                                            \n",
    "                                            # Check if the current hand gesture is HIGH-FIVE.\n",
    "                                            # elif current_gesture == 'HIGH-FIVE':\n",
    "                                            #     distance = calculateDistance(frame, size_gesture_tip_pts['PINKY'],\n",
    "                                            #                          size_gesture_tip_pts['THUMB'], display=False)\n",
    "                                            #     # Get the interpolation function and calculate the gamma scale factor.\n",
    "                                            #     gamma_interp_f = interp1d([30,230], [min_gamma, max_gamma])\n",
    "                                            #     gamma_scale = gamma_interp_f(distance)\n",
    "                                                \n",
    "                                            #     # Update the current mode and change state.\n",
    "                                            #     current_mode = 'GAMMA CORRECTION'\n",
    "                                            #     change_gamma = True\n",
    "                                            \n",
    "                                            # Get the interpolation function and calculate the bar value.\n",
    "                                            # This will be used to draw a filled rectangle of height varying with the distance.\n",
    "                                            distance = calculateDistance(frame, size_gesture_tip_pts['INDEX'],\n",
    "                                                size_gesture_tip_pts['THUMB'], display=False)\n",
    "                                            bar_interp_f = interp1d([30,230],  [frame_height-50, frame_height-400])\n",
    "                                            bar_value = bar_interp_f(distance)  \n",
    "                                            \n",
    "                                            # Write the current mode on the frame.\n",
    "                                            cv2.putText(frame, f'{current_mode} {(distance-30)//2}%',\n",
    "                                                        (frame_width//2-(20*len(current_mode)), 40),\n",
    "                                                        cv2.FONT_HERSHEY_PLAIN, 3, (0, 255, 0), 3)\n",
    "                                            \n",
    "                                            # Draw the filled rectangle with varying height on the frame.\n",
    "                                            cv2.rectangle(frame, (frame_width-80, int(bar_value)), \n",
    "                                                        (frame_width-50, frame_height-50), (255, 0, 255), -1)\n",
    "                                            \n",
    "                                            # Draw another rectangle around the filled rectangle on the frame.\n",
    "                                            cv2.rectangle(frame, (frame_width-80, frame_height-400),\n",
    "                                                        (frame_width-50, frame_height-50), (0, 255, 0), 6)\n",
    "                            # Otherwise.\n",
    "                            else:\n",
    "                                # Clear the buffer.\n",
    "                                buffer.clear()\n",
    "                                \n",
    "                            # Check if the change brighness state is true.\n",
    "                            if change_brightness:\n",
    "                                \n",
    "                                # Change the brighness of the frame. \n",
    "                                frame = changeSatValue(frame, scale_factor=brightness_scale,\n",
    "                                                    channel='Value', display=False)\n",
    "                            \n",
    "                            # Check if the change saturation state is true.\n",
    "                            if change_saturation:\n",
    "                                \n",
    "                                # Change the saturation of the frame. \n",
    "                                frame = changeSatValue(frame, scale_factor=saturation_scale,\n",
    "                                                    channel='Saturation', display=False) \n",
    "                            \n",
    "                            # Check if the change contrast state is true.\n",
    "                            if change_contrast:\n",
    "                                \n",
    "                                # Change the contrast of the frame. \n",
    "                                frame = changeContrast(frame, contrast_scale, display=False)\n",
    "                            \n",
    "                            # Check if the change gamma state is true.\n",
    "                            # if change_gamma:\n",
    "                                \n",
    "                            #     # Perform gamma correction on the frame. \n",
    "                            #     frame = gammaCorrection(frame, gamma_scale, display=False)\n",
    "                            \n",
    "                            # Display the frame.\n",
    "                            cv2.imshow('Live Gesture Control Application', frame)\n",
    "                            \n",
    "                            # Wait for 1ms. If a key is pressed, retreive the ASCII code of the key.\n",
    "                            k = cv2.waitKey(1) & 0xFF\n",
    "                            \n",
    "                            # Check if 'ESC' is pressed and break the loop.\n",
    "                            if(k == 27):\n",
    "                                break\n",
    "\n",
    "                        # Release the VideoCapture Object and close the windows.\n",
    "                        camera_video.release()\n",
    "                        cv2.destroyAllWindows()\n",
    "                        \n",
    "                    \n",
    "                    elif current_gesture == 'SPIDERMAN':\n",
    "                        # Initialize the mediapipe hands class.\n",
    "                        mp_hands = mp.solutions.hands\n",
    "\n",
    "                        # Set up the Hands functions for videos.\n",
    "                        hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, \n",
    "                                            min_detection_confidence=0.8, min_tracking_confidence=0.8)\n",
    "\n",
    "                        # Initialize the mediapipe drawing class.\n",
    "                        mp_drawing = mp.solutions.drawing_utils\n",
    "                        # Initialize the VideoCapture object to read from the webcam.\n",
    "                        camera_video = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "                        camera_video.set(3,1280)\n",
    "                        camera_video.set(4,960)\n",
    "\n",
    "                        # Create named window for resizing purposes.\n",
    "                        cv2.namedWindow('Hand Paint', cv2.WINDOW_NORMAL)\n",
    "\n",
    "                        # Initialize variables to store previous x and y location.\n",
    "                        # That are hand brush x and y coordinates in the previous frame.\n",
    "                        prev_x = None \n",
    "                        prev_y = None\n",
    "\n",
    "                        # Initialize a canvas to draw on.\n",
    "                        canvas = np.zeros(shape=(int(camera_video.get(cv2.CAP_PROP_FRAME_HEIGHT)),\n",
    "                                                int(camera_video.get(cv2.CAP_PROP_FRAME_WIDTH)), 3),\n",
    "                                        dtype=np.uint8)\n",
    "\n",
    "                        # Initialize a variable to store the color value.\n",
    "                        paint_color = 0, 255, 0\n",
    "\n",
    "                        # Initialize a variable to store the buffer length.\n",
    "                        BUFFER_MAX_LENGTH = 10\n",
    "\n",
    "                        # Initialize a buffer to store recognized gestures.\n",
    "                        buffer = deque([], maxlen=BUFFER_MAX_LENGTH)\n",
    "\n",
    "                        # Initialize a variable to store the hand label for gesture recognition.\n",
    "                        hand_label=\"RIGHT\"\n",
    "\n",
    "                        # Initialize a variable to store the erase mode.\n",
    "                        erase_mode = False\n",
    "                        DEFAULT_SHAPE_SIZE = 100\n",
    "\n",
    "                                            # Initialize a variable to store the shape size.\n",
    "                        shape_size = DEFAULT_SHAPE_SIZE\n",
    "\n",
    "                                            # Initialize a variable to store the selected shape. \n",
    "                        shape_selected = None\n",
    "\n",
    "                        # Iterate until the webcam is accessed successfully.\n",
    "                        while camera_video.isOpened():\n",
    "                        \n",
    "                            # Read a frame.\n",
    "                            ok, frame = camera_video.read()\n",
    "                            \n",
    "                            # Check if frame is not read properly then \n",
    "                            # continue to the next iteration to read the next frame.\n",
    "                            if not ok:\n",
    "                                continue\n",
    "                            \n",
    "                            # Flip the frame horizontally for natural (selfie-view) visualization.\n",
    "                            frame = cv2.flip(frame, 1)\n",
    "\n",
    "                            # Get the height and width of the frame of the webcam video.\n",
    "                            frame_height, frame_width, _ = frame.shape\n",
    "                            \n",
    "                            # Perform Hands landmarks detection on the frame.\n",
    "                            frame, results = detectHandsLandmarks(frame, hands, draw=True, display=False)\n",
    "                            \n",
    "                            # Check if the hands landmarks in the frame are detected.\n",
    "                            if results.multi_hand_landmarks:\n",
    "                                \n",
    "                                # Perform a hand gesture recognition.\n",
    "                                # I have modified this recognizeGestures() function,\n",
    "                                # to return the fingers tips position of the both hands.\n",
    "                                current_gesture, hands_tips_positions = recognizeGestures(frame, results,\n",
    "                                                                                        hand_label=\"RIGHT\", draw=False,\n",
    "                                                                                        display=False)\n",
    "                                # Check if a known gesture is recognized.\n",
    "                                if current_gesture != 'UNKNOWN':\n",
    "                                    if current_gesture==\"INDEX POINTING UP\":\n",
    "                                    \n",
    "                                        # Check if all the gestures stored in the buffer are equal to the current gesture.\n",
    "                                        if all(current_gesture==gesture for gesture in buffer):\n",
    "                                            \n",
    "                                            # Append the current gesture into the buffer.\n",
    "                                            buffer.append(current_gesture)\n",
    "                                            \n",
    "                                        # Otherwise.\n",
    "                                        else:\n",
    "                                            \n",
    "                                            # Clear the buffer.\n",
    "                                            buffer.clear()\n",
    "                                        \n",
    "                                        # Check if the length of the buffer is equal to the maxlength, that is 10.\n",
    "                                        if len(buffer) == BUFFER_MAX_LENGTH:\n",
    "                                            print(hands_tips_positions)\n",
    "                                            # Draw, Erase or Clear the canvas depending upon the current gesture.\n",
    "                                            canvas, (prev_x, prev_y) = draw(frame, canvas, current_gesture,\n",
    "                                                                            hands_tips_positions,\n",
    "                                                                            (prev_x, prev_y), paint_color)\n",
    "\n",
    "                                        # Otherwise.\n",
    "                                        else:\n",
    "\n",
    "                                            # Reset, by updating the previous x and y values to None.\n",
    "                                            # This is required to start a new drawing.\n",
    "                                            prev_x, prev_y = None, None\n",
    "                                    \n",
    "                                    \n",
    "                                    \n",
    "                                    if current_gesture == 'VICTORY':\n",
    "                                                                \n",
    "                                                                # Check if a shape is selected.\n",
    "                                                                if shape_selected:\n",
    "                                                            \n",
    "                                                                    # Calculate the distance between the middle finger tip and thumb tip landmark of the other hand.\n",
    "                                                                    distance = calculateDistance(frame, hands_tips_positions[\"MIDDLE\"],hands_tips_positions[\"THUMB\"], display=False)\n",
    "\n",
    "                                                                    # Check if the distance is calculated successfully.\n",
    "                                                                    # This will be none in case when the hand is not in the frame.\n",
    "                                                                    if distance:\n",
    "\n",
    "                                                                        # Get the interpolation function and calculate the new shape size value.\n",
    "                                                                        shape_size_interp_f = interp1d([30,230], [50, 200])\n",
    "                                                                        shape_size = int(shape_size_interp_f(distance))\n",
    "\n",
    "                                                                        # Write the current distance percentage on the frame.\n",
    "                                                                        cv2.putText(img=frame, text=f'{int((distance-30)/2)}%',org=(frame_width-130, frame_height-430), \n",
    "                                                                                    fontFace=cv2.FONT_HERSHEY_PLAIN, fontScale=3, color=paint_color, thickness=3)\n",
    "\n",
    "\n",
    "                                                                        # Get the interpolation function and calculate the bar value.\n",
    "                                                                        # This will be used to draw a filled rectangle of height varying with the distance.\n",
    "                                                                        bar_interp_f = interp1d([30,230],  [frame_height-50, frame_height-400])\n",
    "                                                                        bar_value = bar_interp_f(distance)  \n",
    "\n",
    "\n",
    "                                                                        # Draw the filled rectangle with varying height on the frame.\n",
    "                                                                        cv2.rectangle(frame, (frame_width-80, int(bar_value)), \n",
    "                                                                                    (frame_width-50, frame_height-50), (255, 0, 255), -1)\n",
    "\n",
    "                                                                        # Draw another rectangle around the filled rectangle on the frame.\n",
    "                                                                        cv2.rectangle(frame, (frame_width-80, frame_height-400),\n",
    "                                                                                    (frame_width-50, frame_height-50), (0, 255, 0), 6)\n",
    "\n",
    "                                                                # Otherwise.\n",
    "                                                                else:\n",
    "                                                                    \n",
    "                                                                    # Select a shape utilizing hand landmarks.\n",
    "                                                                    shape_selected = selectShape(frame, hands_tips_positions,\n",
    "                                                                                                DEFAULT_SHAPE_SIZE)\n",
    "                                                            \n",
    "                                                            # Draw the selected shape with center at the tip of the middle finger of the hand.\n",
    "                                frame, canvas, shape_selected = drawShapes(frame, canvas, shape_selected, current_gesture,\n",
    "                                                                                                    hands_tips_positions, paint_color, \n",
    "                                                                                                    shape_size)            \n",
    "                                    \n",
    "                            # Otherwise.\n",
    "                            else:\n",
    "                                \n",
    "                                # Clear the buffer.\n",
    "                                buffer.clear()\n",
    "                                \n",
    "                            # Write instructions to switch hand for gesture recognition on the frame.\n",
    "                            cv2.putText(img=frame, text=f'{hand_label} hand selected, press s to switch.',\n",
    "                                        org=(10, 30), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=paint_color,\n",
    "                                        thickness=2)\n",
    "\n",
    "                            # Update the pixel values of the frame with the canvas's values at the indexes where canvas!=0\n",
    "                            # i.e. where canvas is not black and something is drawn there.\n",
    "                            # In short, this will copy the drawings from canvas to the frame.\n",
    "                            frame[np.mean(canvas, axis=2)!=0] = canvas[np.mean(canvas, axis=2)!=0]\n",
    "                            \n",
    "                            # Display the frame.\n",
    "                            cv2.imshow(\"Hand Paint\", frame)\n",
    "                            \n",
    "                        # Wait for 1ms. If a key is pressed, retreive the ASCII code of the key.\n",
    "                            k = cv2.waitKey(1) & 0xFF\n",
    "                            \n",
    "                            # Check if 'ESC' is pressed and break the loop.\n",
    "                            if k == 27:\n",
    "                                break\n",
    "                            \n",
    "                            # Check if 's' key is pressed and switch the hand label.\n",
    "                            elif k == ord('s'):\n",
    "                                \n",
    "                                # Set gesture hand label to 'LEFT', if it was 'RIGHT',\n",
    "                                # Otherwise if it was 'LEFT', set it to 'RIGHT'.\n",
    "                                hand_label = 'LEFT' if hand_label == 'RIGHT' else 'RIGHT'\n",
    "                                \n",
    "                        # Release the VideoCapture Object and close the windows.\n",
    "                        camera_video.release()\n",
    "                        cv2.destroyAllWindows()\n",
    "                    \n",
    "                    # Check if the current hand gesture is SPIDERMAN.\n",
    "                    elif current_gesture == 'THREE':\n",
    "                        # Initialize the mediapipe pose class.\n",
    "                        mp_pose = mp.solutions.pose\n",
    "\n",
    "                        # Set up the pose landmarks function for videos.\n",
    "                        pose_videos = mp_pose.Pose(static_image_mode=False, model_complexity=1, smooth_landmarks=True, \n",
    "                                                enable_segmentation=True, smooth_segmentation=True, \n",
    "                                                min_detection_confidence=0.5, min_tracking_confidence=0.8)\n",
    "                        # Specify the path where the dataset is stored.\n",
    "                        DATASET_DIR = 'dataset'\n",
    "\n",
    "                        # Specify the number of frames of a video that will be fed to the model as one sequence.\n",
    "                        sequence_length = 30 \n",
    "\n",
    "                        # Initialize the list containing the classes on which we want to train our model.\n",
    "                        classes_list = [\"Hello\", \"bye\", \"Thankyou\"]\n",
    "                        # Initialize the VideoCapture object to read from the webcam.\n",
    "                        camera_video = cv2.VideoCapture(0)\n",
    "                        camera_video.set(3,1280)\n",
    "                        camera_video.set(4,960)\n",
    "\n",
    "                        # Create named window for resizing purposes.\n",
    "                        cv2.namedWindow('Sign Language Recognition', cv2.WINDOW_NORMAL)\n",
    "\n",
    "                        # Load the model from disk.\n",
    "                        loaded_model = load_model('Model/sign_language_recognizer.h5')\n",
    "\n",
    "                        # Initialize a deque to store the frames.\n",
    "                        sequence = deque([], maxlen=sequence_length)\n",
    "\n",
    "                        # Initialize a variable to store the buffer length.\n",
    "                        BUFFER_MAX_LENGTH = 20\n",
    "\n",
    "                        # Initialize a buffer to store the prediction results.\n",
    "                        buffer = deque([], maxlen=BUFFER_MAX_LENGTH)\n",
    "\n",
    "                        # Initialize a variable to store the minimum prediction confidence.\n",
    "                        threshold = 0.55\n",
    "\n",
    "                        # Iterate until the webcam is accessed successfully.\n",
    "                        while camera_video.isOpened():\n",
    "\n",
    "                            # Read a frame.\n",
    "                            ok, frame = camera_video.read()\n",
    "\n",
    "                            # Check if frame is not read properly then\n",
    "                            # continue to the next iteration to read the next frame.\n",
    "                            if not ok:\n",
    "                                continue\n",
    "\n",
    "                            # Flip the frame horizontally for natural (selfie-view) visualization.\n",
    "                            frame = cv2.flip(frame, 1)\n",
    "\n",
    "                            # Get the height and width of the frame of the webcam video.\n",
    "                            frame_height, frame_width, _ = frame.shape\n",
    "\n",
    "                            # Extract the required pose keypoints of the person in the frame.\n",
    "                            frame, extracted_keypoints = extractPoseKeypoints(frame, pose_videos)\n",
    "\n",
    "                            # Print the shape of extracted_keypoints for debugging.\n",
    "                            print(\"Shape of extracted_keypoints:\", extracted_keypoints.shape)\n",
    "\n",
    "                            # Only append the extracted keypoints into the deque if it's not empty.\n",
    "                            if extracted_keypoints.shape != (0,):\n",
    "                                sequence.append(extracted_keypoints)\n",
    "\n",
    "                            # Print the length of sequence for debugging.\n",
    "                            print(\"Length of sequence:\", len(sequence))\n",
    "\n",
    "                            # Check if the length of the deque is equal to the required sequence length.\n",
    "                            if len(sequence) == sequence_length:\n",
    "\n",
    "                                # Predict the Sign, the person is making in the sequence.\n",
    "                                prediction_probs = loaded_model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "\n",
    "                                # Check if the highest predicted probability of a class is > the threshold value.\n",
    "                                if prediction_probs[np.argmax(prediction_probs)] > threshold:\n",
    "\n",
    "                                    # Get the class (Sign) name which has the highest predicted probability.\n",
    "                                    current_sign = classes_list[np.argmax(prediction_probs)]\n",
    "\n",
    "                                    # Append the predicted class into the buffer.\n",
    "                                    buffer.append(current_sign)\n",
    "\n",
    "                                # Otherwise.\n",
    "                                else:\n",
    "\n",
    "                                    # Check if the length of the buffer is > zero.\n",
    "                                    if len(buffer) > 0:\n",
    "\n",
    "                                        # Remove an element from the left side of the deque.\n",
    "                                        buffer.popleft()\n",
    "\n",
    "                            # Check if the length of the buffer is equal to the maxlength.\n",
    "                            if len(buffer) == BUFFER_MAX_LENGTH:\n",
    "\n",
    "                                # Write the predicted class on the frame.\n",
    "                                cv2.putText(frame, f'{max(buffer).upper()}!', (int(frame_width/2)-(len(max(buffer))*50),\n",
    "                                                                            int(frame_height/2)+100),\n",
    "                                            cv2.FONT_HERSHEY_PLAIN, 10, (0,255,0), 7)\n",
    "\n",
    "                            # Display the frame.\n",
    "                            cv2.imshow(\"Sign Language Recognition\", frame)\n",
    "\n",
    "                            # Wait for 1ms.\n",
    "                            # If a key is pressed, retrieve the ASCII code of the key.\n",
    "                            k = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "                            # Check if 'ESC' is pressed and break the loop.\n",
    "                            if(k == 27):\n",
    "                                break\n",
    "\n",
    "                        # Release the VideoCapture Object and close the windows.\n",
    "                        camera_video.release()\n",
    "                        cv2.destroyAllWindows()\n",
    "\n",
    "                    \n",
    "                    # Check if the current hand gesture is INDEX POINTING UP.\n",
    "                    if current_gesture == 'FOURS':\n",
    "                        # Initialize the mediapipe hands class.\n",
    "                        mp_hands = mp.solutions.hands\n",
    "                        print(mp_hands)\n",
    "                        # Set up the Hands functions for images and videos.\n",
    "                        hands = mp_hands.Hands(static_image_mode=True, max_num_hands=2, min_detection_confidence=0.3)\n",
    "                        hands_videos = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.8)\n",
    "\n",
    "                        # Initialize the mediapipe drawing class.\n",
    "                        mp_drawing = mp.solutions.drawing_utils\n",
    "                        # Initialize the VideoCapture object to read from the webcam.\n",
    "                        camera_video = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "                        camera_video.set(3,1280)\n",
    "                        camera_video.set(4,960)\n",
    "                        website_opened1=False\n",
    "                        website_opened2=False\n",
    "                        # Create named window for resizing purposes.\n",
    "                        cv2.namedWindow('Open Application', cv2.WINDOW_NORMAL)\n",
    "\n",
    "                        # Initialize the minimum brightness and saturation scale factors.\n",
    "                        min_brightness = min_saturation = 0\n",
    "\n",
    "                        # Initialize the maximum brightness and saturation scale factors.\n",
    "                        max_brightness = max_saturation = 3\n",
    "\n",
    "                        # Initialize the minimum and maximum contrast scale factors. \n",
    "                        min_contrast=1\n",
    "                        max_contrast=3\n",
    "\n",
    "                        # Initialize the minimum and maximum gamma scale factors.\n",
    "                        min_gamma=0.1\n",
    "                        max_gamma=3\n",
    "\n",
    "                        # Initialize a buffer to store recognized gestures.\n",
    "                        buffer = deque([], maxlen=20)\n",
    "\n",
    "                        # Initialize some variables to store the change state.\n",
    "                        change_brightness = change_saturation = change_contrast = change_gamma = False\n",
    "\n",
    "                        # Initialize a variable to store the current mode.\n",
    "                        current_mode = ''\n",
    "\n",
    "                        print(f\"camera_video.isOpened()={camera_video.isOpened()}\")\n",
    "                        # Iterate until the webcam is accessed successfully.\n",
    "                        while camera_video.isOpened():\n",
    "                        \n",
    "                            # Read a frame.\n",
    "                            ok, frame = camera_video.read()\n",
    "                            \n",
    "                            # Check if frame is not read properly then continue to the next iteration to read the next frame.\n",
    "                            if not ok:\n",
    "                        #         print(\"Here\")\n",
    "                                continue\n",
    "                            \n",
    "                            # Flip the frame horizontally for natural (selfie-view) visualization.\n",
    "                            frame = cv2.flip(frame, 1)\n",
    "                            \n",
    "                            # Get the height and width of the frame of the webcam video.\n",
    "                            frame_height, frame_width, _ = frame.shape\n",
    "                            \n",
    "                            # Perform Hands landmarks detection on the frame.\n",
    "                            _, results = detectHandsLandmarks(frame, hands_videos, draw=True, display=False)\n",
    "                            \n",
    "                            # Check if the hands landmarks in the frame are detected.\n",
    "                            if results.multi_hand_landmarks:\n",
    "                            \n",
    "                                # Perform left hand gesture recognition.\n",
    "                                current_gesture, size_gesture_tip_pts = recognizeGestures(frame, results,\n",
    "                                                                                        hand_label='RIGHT',\n",
    "                                                                                        draw=False, display=False)\n",
    "                                \n",
    "                                # Check if a known gesture is recognized.\n",
    "                                if current_gesture != 'UNKNOWN':\n",
    "                                    print(f\"current_gesture={current_gesture}\")\n",
    "                                    # Check if all the gestures stored in the buffer are equal to the current gesture.\n",
    "                                    if all(current_gesture==gesture for gesture in buffer):\n",
    "                                        \n",
    "                                        # Append the current gesture into the buffer.\n",
    "                                        buffer.append(current_gesture)\n",
    "                                        \n",
    "                                    # Otherwise.\n",
    "                                    else:\n",
    "                                        \n",
    "                                        # Clear the buffer.\n",
    "                                        buffer.clear()\n",
    "                                    \n",
    "                                    # Check if the length of the buffer is equal to 20.\n",
    "                                    if len(buffer) == 20:\n",
    "                                        \n",
    "                                        # Calculate the distance between the middle finger tip and thumb tip landmark of the right hand.\n",
    "                                        # distance = calculateDistance(frame, size_gesture_tip_pts['INDEX'],\n",
    "                                        #                              size_gesture_tip_pts['THUMB'], display=False)\n",
    "                                        # print(distance)\n",
    "                                        # Check if the distance is calculated successfully.\n",
    "                                        # This will be none in case when the hand is not in the frame.\n",
    "                                        if current_gesture != 'UNKNOWN':\n",
    "                                            \n",
    "                                            # Check if the current hand gesture is INDEX POINTING UP.\n",
    "                                            if current_gesture == 'INDEX POINTING UP' and not notepad_opened:\n",
    "                                                subprocess.Popen([\"notepad.exe\"])\n",
    "                                                notepad_opened=True\n",
    "                                            \n",
    "                                            elif current_gesture != 'INDEX POINTING UP':\n",
    "                                                    notepad_opened = False\n",
    "                                                    \n",
    "                                            # Check if the current hand gesture is VICTORY.\n",
    "                                            if current_gesture == 'VICTORY' and not chrome_opened:\n",
    "                                                subprocess.Popen([\"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\"])\n",
    "                                                chrome_opened=True\n",
    "                                            \n",
    "                                            elif current_gesture != 'VICTORY':\n",
    "                                                chrome_opened=False\n",
    "                                            \n",
    "                                            # Check if the current hand gesture is SPIDERMAN.\n",
    "                                            if current_gesture == 'THREE' and not calc_opened:\n",
    "                                                subprocess.Popen([\"calc.exe\"])\n",
    "                                                calc_opened_opened=True\n",
    "                                            \n",
    "                                            elif current_gesture != 'THREE':\n",
    "                                                calc_opened=False\n",
    "                                                \n",
    "                                            \n",
    "                                            if current_gesture == 'HIGH-FIVE' and not website_opened1:\n",
    "                                                webbrowser.open('https://www.flipkart.com')\n",
    "                                                website_opened1 = True  # Set to True to avoid reopening the website in the loop\n",
    "                                    \n",
    "                                                # Reset website_opened to False if a different gesture is recognized\n",
    "                                            elif current_gesture != 'HIGH-FIVE':\n",
    "                                                    website_opened1 = False\n",
    "                                            \n",
    "                                            if current_gesture == 'FOURS' and not website_opened2:\n",
    "                                                webbrowser.open('https://www.youtube.com')\n",
    "                                                website_opened2 = True  # Set to True to avoid reopening the website in the loop\n",
    "                                    \n",
    "                                                # Reset website_opened to False if a different gesture is recognized\n",
    "                                            elif current_gesture != 'FOURS':\n",
    "                                                    website_opened2 = False\n",
    "                                            \n",
    "                                            # Get the interpolation function and calculate the bar value.\n",
    "                                            # This will be used to draw a filled rectangle of height varying with the distance.\n",
    "                                            distance = calculateDistance(frame, size_gesture_tip_pts['INDEX'],\n",
    "                                                size_gesture_tip_pts['THUMB'], display=False)\n",
    "                                            bar_interp_f = interp1d([30,230],  [frame_height-50, frame_height-400])\n",
    "                                            bar_value = bar_interp_f(distance)  \n",
    "                                            \n",
    "                                            # Write the current mode on the frame.\n",
    "                                            cv2.putText(frame, f'{current_mode} {(distance-30)//2}%',\n",
    "                                                        (frame_width//2-(20*len(current_mode)), 40),\n",
    "                                                        cv2.FONT_HERSHEY_PLAIN, 3, (0, 255, 0), 3)\n",
    "                                            \n",
    "                                            # Draw the filled rectangle with varying height on the frame.\n",
    "                                            cv2.rectangle(frame, (frame_width-80, int(bar_value)), \n",
    "                                                        (frame_width-50, frame_height-50), (255, 0, 255), -1)\n",
    "                                            \n",
    "                                            # Draw another rectangle around the filled rectangle on the frame.\n",
    "                                            cv2.rectangle(frame, (frame_width-80, frame_height-400),\n",
    "                                                        (frame_width-50, frame_height-50), (0, 255, 0), 6)\n",
    "                            # Otherwise.\n",
    "                            else:\n",
    "                                # Clear the buffer.\n",
    "                                buffer.clear()\n",
    "                                \n",
    "        \n",
    "                            \n",
    "                            # Check if the change gamma state is true.\n",
    "                            # if change_gamma:\n",
    "                                \n",
    "                            #     # Perform gamma correction on the frame. \n",
    "                            #     frame = gammaCorrection(frame, gamma_scale, display=False)\n",
    "                            \n",
    "                            # Display the frame.\n",
    "                            cv2.imshow('Open Application', frame)\n",
    "                            \n",
    "                            # Wait for 1ms. If a key is pressed, retreive the ASCII code of the key.\n",
    "                            k = cv2.waitKey(1) & 0xFF\n",
    "                            \n",
    "                            # Check if 'ESC' is pressed and break the loop.\n",
    "                            if(k == 27):\n",
    "                                break\n",
    "\n",
    "                        # Release the VideoCapture Object and close the windows.\n",
    "                        camera_video.release()\n",
    "                        cv2.destroyAllWindows()\n",
    "                    # if current_gesture == 'HIGH-FIVE' and not website_opened1:\n",
    "                    #     # webbrowser.open('https://www.flipkart.com')\n",
    "                    #     # website_opened1 = True  # Set to True to avoid reopening the website in the loop\n",
    "                    #     subprocess.Popen(\"C:\\\\Program Files\\\\Google\\\\Chrome\\\\Application\\\\chrome.exe\")\n",
    "                    #     # subprocess.Popen([\"chrome.exe\"])\n",
    "                    #     # Reset website_opened to False if a different gesture is recognized\n",
    "                    # elif current_gesture != 'HIGH-FIVE':\n",
    "                    #         website_opened1 = False\n",
    "                    \n",
    "                    # if current_gesture == 'FOURS' and not website_opened2:\n",
    "                    #     subprocess.Popen(['calc.exe'])\n",
    "                    #     website_opened2 = True  # Set to True to avoid reopening the website in the loop\n",
    "            \n",
    "                    #     # Reset website_opened to False if a different gesture is recognized\n",
    "                    # elif current_gesture != 'FOURS':\n",
    "                    #         website_opened2 = False\n",
    "                    \n",
    "                    # # Get the interpolation function and calculate the bar value.\n",
    "                    # # This will be used to draw a filled rectangle of height varying with the distance.\n",
    "                    # distance = calculateDistance(frame, size_gesture_tip_pts['INDEX'],\n",
    "                    #     size_gesture_tip_pts['THUMB'], display=False)\n",
    "                    # bar_interp_f = interp1d([30,230],  [frame_height-50, frame_height-400])\n",
    "                    # bar_value = bar_interp_f(distance)  \n",
    "                    \n",
    "                    # # Write the current mode on the frame.\n",
    "                    # cv2.putText(frame, f'{current_mode} {(distance-30)//2}%',\n",
    "                    #             (frame_width//2-(20*len(current_mode)), 40),\n",
    "                    #             cv2.FONT_HERSHEY_PLAIN, 3, (0, 255, 0), 3)\n",
    "                    \n",
    "                    # # Draw the filled rectangle with varying height on the frame.\n",
    "                    # cv2.rectangle(frame, (frame_width-80, int(bar_value)), \n",
    "                    #               (frame_width-50, frame_height-50), (255, 0, 255), -1)\n",
    "                    \n",
    "                    # # Draw another rectangle around the filled rectangle on the frame.\n",
    "                    # cv2.rectangle(frame, (frame_width-80, frame_height-400),\n",
    "                    #               (frame_width-50, frame_height-50), (0, 255, 0), 6)\n",
    "    # Otherwise.\n",
    "    else:\n",
    "        # Clear the buffer.\n",
    "        buffer.clear()\n",
    "        \n",
    "    # # Check if the change brighness state is true.\n",
    "    # if change_brightness:\n",
    "        \n",
    "    #     # Change the brighness of the frame. \n",
    "    #     frame = changeSatValue(frame, scale_factor=brightness_scale,\n",
    "    #                            channel='Value', display=False)\n",
    "    \n",
    "    # # Check if the change saturation state is true.\n",
    "    # if change_saturation:\n",
    "        \n",
    "    #     # Change the saturation of the frame. \n",
    "    #     frame = changeSatValue(frame, scale_factor=saturation_scale,\n",
    "    #                            channel='Saturation', display=False) \n",
    "    \n",
    "    # # Check if the change contrast state is true.\n",
    "    # if change_contrast:\n",
    "        \n",
    "    #     # Change the contrast of the frame. \n",
    "    #     frame = changeContrast(frame, contrast_scale, display=False)\n",
    "    \n",
    "    # # Check if the change gamma state is true.\n",
    "    # # if change_gamma:\n",
    "        \n",
    "    # #     # Perform gamma correction on the frame. \n",
    "    # #     frame = gammaCorrection(frame, gamma_scale, display=False)\n",
    "    \n",
    "    # Display the frame.\n",
    "    cv2.imshow('Live Gesture Control Application', frame)\n",
    "    \n",
    "    # Wait for 1ms. If a key is pressed, retreive the ASCII code of the key.\n",
    "    k = cv2.waitKey(1) & 0xFF\n",
    "    \n",
    "    # Check if 'ESC' is pressed and break the loop.\n",
    "    if(k == 27):\n",
    "        break\n",
    "\n",
    "# Release the VideoCapture Object and close the windows.\n",
    "camera_video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finalYear",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
